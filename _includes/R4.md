#### <span style="color:#ff201e">Part 4: Predictive Power of Distance Metrics for Link Sentiment</span>

<div class="section" style="margin-top:18px;">

<h4>Research question (R4)</h4>
<p>
Which of the three similarity metrics (shared authorship, stylometric, psychological) best predicts whether an inter-community hyperlink is positive or negative?
</p>

<p>
Plan: (1) compare single feature models, (2) test a combined model, (3) check robustness to class imbalance, and (4) compare logistic regression vs Random Forest.
</p>

<hr style="margin:18px 0;">

### <span style="color:#ff201e">4.1 Comparing predictive power (overview)</span>

<p>
We evaluate three single feature classifiers (one per distance metric), a combined classifier (all three distances), and two model families: <em>Logistic Regression</em> and <em>Random Forest</em>.  
Performance metrics: <b>F1</b> (harmonic mean of precision & recall) and <b>ROC-AUC</b> (discrimination ability).
</p>

<div style="display:flex; gap:12px; margin-top:12px; align-items:center;">
  <div style="background:#fff7ed; padding:12px; border-radius:8px;">
    <strong>Hypothesis H11</strong><br>
    Shared authorship will be the strongest single predictor.
  </div>
  <div style="background:#eef7ff; padding:12px; border-radius:8px;">
    <strong>Hypothesis H12</strong><br>
    A model combining all three distances will outperform single feature models.
  </div>
</div>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.2 Logistic regression — formulas & setup</span>

<p>We train logistic regression models with stratified k-fold cross-validation and report mean ± std for F1 and ROC-AUC.</p>

<!-- formula boxes -->
<div style="display:flex; gap:16px; flex-wrap:wrap; margin-top:12px;">
  <div style="background:#f6f9fa; padding:12px; border-radius:8px; min-width:300px;">
    <div style="font-weight:600; margin-bottom:6px;">F1 score</div>
    <pre style="margin:0; font-family:monospace;">
F1 = 2 * (Precision * Recall) / (Precision + Recall)
    </pre>
  </div>

  <div style="background:#f6f9fa; padding:12px; border-radius:8px; min-width:300px;">
    <div style="font-weight:600; margin-bottom:6px;">Precision &amp; Recall</div>
    <pre style="margin:0; font-family:monospace;">
Precision = TP / (TP + FP)
Recall    = TP / (TP + FN)
    </pre>
  </div>

  <div style="background:#f6f9fa; padding:12px; border-radius:8px; min-width:300px;">
    <div style="font-weight:600; margin-bottom:6px;">Logistic model (probability)</div>
    <pre style="margin:0; font-family:monospace;">
P(y=1|X) = sigmoid(β0 + β1·x1 + β2·x2 + β3·x3)
sigmoid(z) = 1 / (1 + exp(-z))
    </pre>
  </div>
</div>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.2 Results — logistic regression (imbalanced data)</span>

<div style="display:flex; gap:18px; flex-wrap:wrap; margin-top:8px;">
  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>Psychological distance</strong><br>
    F1: <b>0.206 ± 0.001</b><br>
    AUC: <b>0.584 ± 0.003</b>
  </div>

  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>Authors distance</strong><br>
    F1: <b>0.219 ± 0.001</b><br>
    AUC: <b>0.606 ± 0.002</b>
  </div>

  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>Stylometric distance</strong><br>
    F1: <b>0.171 ± 0.002</b><br>
    AUC: <b>0.511 ± 0.003</b>
  </div>
</div>

<div style="margin-top:12px; font-size:0.95rem;">
  <p><b>Interpretation</b></p>
  <ul>
    <li>The shared authorship (authors_cos_dist) is the best single predictor (highest F1 and AUC), but AUC ≈ 0.605 is still only in the fail-to-poor range.</li>
    <li>Psychological distance gives moderate, consistent signal (AUC ≈ 0.58).</li>
    <li>Stylometric distance provides no meaningful discrimination (AUC ≈ 0.51, near random).</li>
  </ul>
</div>

<div style="text-align:center; margin-top:14px;">
  <img src="assets/img/R4/logreg_f1_auc_single_features.png" width="70%" alt="LogReg F1 and AUC per feature">
  <div style="font-size:0.85rem;"><em>Left: F1 per feature. Right: ROC-AUC per feature.</em></div>
</div>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.3 Downsampling (balanced classes)</span>

<p>Class counts: positive = 677,777; negative = 76,259 → imbalance ≈ 89:11 (negatives ≈ 8.9%). We downsample the majority class to achieve 50/50 balance and repeat the logistic evaluation.</p>

<div style="display:flex; gap:12px; margin-top:10px;">
  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>Downsampled results</strong><br>
    Psychological: F1 <b>0.569 ± 0.001</b>, AUC <b>0.583 ± 0.002</b><br>
    Authors:       F1 <b>0.589 ± 0.002</b>, AUC <b>0.604 ± 0.001</b><br>
    Stylometric:   F1 <b>0.488 ± 0.003</b>, AUC <b>0.511 ± 0.002</b>
  </div>

  <div style="background:#fff7ef; padding:12px; border-radius:8px;">
    <strong>Key note</strong><br>
    F1 increases substantially after balancing, but ROC-AUC remains practically unchanged. The F1 jump is largely a consequence of balancing the class distribution.
  </div>
</div>

<div style="margin-top:12px;">
  <p><b>Takeaway:</b> downsampling inflates metrics tied to class balance (precision/recall/F1) but does not improve the underlying discriminative ability measured by ROC-AUC.</p>
</div>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.4 Logistic regression : combined models</span>

<p>We compare two logistic configurations: (A) all three distances; (B) only authors + psychological (drop stylometric).</p>

<div style="display:flex; gap:12px; margin-top:10px;">
  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>All three distances</strong><br>
    F1 per fold: [0.2199, 0.2234, 0.2195, 0.2198, 0.2192]<br>
    F1: <b>0.220 ± 0.002</b><br>
    AUC: <b>0.615 ± 0.003</b>
  </div>

  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>Authors + Psychological</strong><br>
    F1 per fold: [0.2174, 0.2199, 0.2160, 0.2174, 0.2169]<br>
    F1: <b>0.218 ± 0.001</b><br>
    AUC: <b>0.610 ± 0.002</b>
  </div>
</div>

<div style="margin-top:12px;">
  <p>
    Including stylometric distance increases the combined model's AUC slightly (0.615 vs 0.610). The gain is small but consistent across folds, suggesting stylometric distance adds marginal complementary information in the logistic setting.
  </p>
</div>

<div style="text-align:center; margin-top:14px">
  <img src="assets/img/R4/logreg_combined_boxplots.png" width="70%" alt="Combined vs authors+psych logistic boxplots">
  <div style="font-size:0.85rem;"><em>Boxplots comparing AUC (and F1) distributions across folds.</em></div>
</div>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.5 Random Forest : combined model</span>

<p>We run a stratified 5-fold Random Forest with all three metrics. Results show a marked improvement over logistic regression.</p>

<div style="display:flex; gap:12px; margin-top:10px;">
  <div style="background:#fff; border:1px solid #e6e9ec; padding:12px; border-radius:8px;">
    <strong>Random Forest (all 3)</strong><br>
    F1 per fold: [0.298, 0.300, 0.295, 0.294, 0.297]<br>
    <b>Mean F1: 0.297 ± 0.002</b><br>
    AUC per fold: [0.723, 0.727, 0.720, 0.722, 0.725]<br>
    <b>Mean AUC: 0.723 ± 0.002</b>
  </div>

  <div style="background:#f0fff4; padding:12px; border-radius:8px;">
    <strong>Improvement</strong><br>
    Compared to logistic regression, Random Forest increases AUC from ≈0.615 to ≈0.723 — a substantial gain in discrimination.
  </div>
</div>

<div style="text-align:center; margin-top:14px;">
  <img src="assets/img/R4/rf_tree_visualization.png" width="72%" alt="Random forest tree visualization">
  <div style="font-size:0.85rem;"><em>Top-level splits from a tree in the forest (illustrative).</em></div>
</div>

<div style="text-align:center; margin-top:14px;">
  <img src="assets/img/R4/rf_roc_curve.png" width="72%" alt="Random forest ROC curve">
  <div style="font-size:0.85rem;"><em>ROC curve and AUC ≈ 0.723 for the Random Forest ensemble.</em></div>
</div>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.6 Feature importance & stylometric usefulness</span>

<p>
We inspect Random Forest feature importances to quantify contributions of each metric.
</p>

<div style="text-align:center; margin-top:12px;">
  <img src="assets/img/R4/feature_importance_rf.png" width="60%" alt="Feature importances">
  <div style="font-size:0.85rem;"><em>Shared authorship is top; psychological and stylometric contribute meaningfully.</em></div>
</div>

<p style="margin-top:10px;">
Summary of findings:
<ul>
  <li><b>Shared authorship</b> is the single most influential feature across models.</li>
  <li><b>Psychological distance</b> provides moderate and stable signal.</li>
  <li><b>Stylometric distance</b> is weak alone, but improves combined model performance slightly (non-negligible importance in Random Forest).</li>
</ul>
</p>

<hr style="margin:18px 0;">


### <span style="color:#ff201e">4.7 Final conclusions (Part 4)</span>

<div style="background:#eef1f5; padding:16px; border-radius:10px;">
  <ol>
    <li><b>Hypothesis H11 validated:</b> shared authorship distance is the best single predictor (highest AUC and F1 among single-feature models).</li>
    <li><b>Hypothesis H12 validated:</b> combining distances improves performance: logistic regression (AUC ≈ 0.615) and Random Forest (AUC ≈ 0.723) outperform single feature models.</li>
    <li><b>Hypothesis H13 rejected:</b> stylometric distance, while weak alone, contributes marginally to combined models and carries non-zero importance in the Random Forest.</li>
  </ol>

  <p style="margin-top:8px;">
    Practical takeaway: authorship distance + psychological distance form a strong baseline. Adding stylometric distance produces a small but consistent lift in ensemble models, and Random Forests extract substantially more signal than linear models in this problem.
  </p>
</div>

</div>
